---
layout: about
title: About
permalink: /
subtitle: # <a href='#'>Affiliations</a>. Address. Contacts. Motto. Etc.

profile:
  align: left
  image: jaylon_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
  #    <p>555 your office number</p>
#    <p>123 your address street</p>
#    <p>Your City, State 12345</p>

news: false # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---


I am a researcher at [Peking University - GALBOT Joint Lab](https://www.galbot.com/)
and [Center on Frontiers of Computing Studies (CFCS)](https://cfcs.pku.edu.cn/english/), directed
by [He Wang](https://hughw19.github.io/). I received my B.S. in Computer Engineering from UC Santa Cruz. I have
previously interned at BAAI, SenseTime, Westlake University, and Tsinghua University, and have also participated
in exchange programs at UC Berkeley and Shanghai Jiao Tong University.

My research interests lie at the intersection of robotics and machine learning, with a specific focus on locomotion and
manipulation tasks. Regarding the **"RL vs. MPC"** debate, I believe the distinction is subtle.
Both frameworks demand human-in-the-loop refinement during development 
While RL employs reward-driven trial-and-error learning, MPC utilizes model-based real-time optimization. 
Crucially, neither evolves autonomously: human expertise fundamentally shapes their architectures, 
hyperparameters, and failure recovery strategies. 